# Dnypro Cascade Hydroelectric Power Stations - Kafka Analytics Platform

**Option 3, Sub-option B (Analytics Focus)**

A project for testing and analyzing Apache Kafka performance for processing data from hydroelectric power stations. The system collects metrics from 15 hydroelectric units, generates synthetic data, and tests various Kafka configurations for analytics optimization.

---

## ðŸ“‹ Project Structure

```
scripts/
â”œâ”€â”€ hydro_data_generator.py      # Synthetic hydroelectric data generator
â”œâ”€â”€ hydro_analytics.py           # Kafka testing and benchmark
â”œâ”€â”€ hydro_test_1000.json         # Sample dataset (1000 records)
â””â”€â”€ README.md
```

---

## ðŸš€ System Components

### 1. **hydro_data_generator.py** - Data Generator
Generates realistic data from hydroelectric power stations of the Dnieper Cascade.

**Generated parameters per unit:**
- `device_id` - unique identifier (HYDRO_DN_001..015)
- `power_output` - generation power (kW)
- `efficiency` - turbine efficiency (%)
- `temperature` - cooling temperature (Â°C)
- `voltage` - voltage (V)
- `current` - current (A)
- `status` - status ("generating", "standby", "maintenance")
- `location` - GPS coordinates
- `maintenance_hours` - operating hours
- `water_flow` - water flow rate (mÂ³/s)
- `water_level` - water level (m)
- `turbine_type` - turbine type ("kaplan", "francis", "pelton")

**Built-in realistic patterns:**
- âœ“ Daily water flow cycles (peak at noon)
- âœ“ Seasonal temperature variations
- âœ“ Probabilistic status transitions (85% generating, 10% standby, 5% maintenance)
- âœ“ Correlation between power and output

**Usage:**
```bash
python hydro_data_generator.py
```

**Result:** Generates `hydro_test_1000.json` with 1000 records in JSONL format (one JSON per line).

---

### 2. **hydro_analytics.py** - Kafka Testing
Complete Kafka performance testing suite with focus on analytics scenarios.

**Test stages:**

#### **STAGE 1: Topic Creation**
Creates 11 specialized topics:
- `hydro-main` (4 partitions, 30 days retention) - main stream
- `hydro-batch-test` (3 partitions) - batch parameter testing
- `hydro-comp-*` (3 partitions each) - compression tests (none, snappy, lz4, zstd)
- `hydro-part-*` (3 partitions each) - partitioning tests (2, 4, 8 partitions)
- `hydro-analytics` (8 partitions, 1 year retention, segment.ms=1 day) - analytics

#### **STAGE 2: BATCH/LINGER Testing**
Tests impact of buffering on throughput:
```
batch.size Ã— linger.ms combinations:
â”œâ”€ 16 KB Ã— 0 ms      â†’ Minimal latency, low throughput
â”œâ”€ 64 KB Ã— 10 ms     
â”œâ”€ 256 KB Ã— 50 ms    â†’ Balanced parameters
â”œâ”€ 512 KB Ã— 100 ms   
â”œâ”€ 1 MB Ã— 200 ms     â†’ Optimal for analytics
â””â”€ 1 MB Ã— 500 ms     â†’ Maximum throughput
```

**Metrics:** throughput (rec/sec), avg/p95/p99 latency (ms), duration

#### **STAGE 3: COMPRESSION Testing**
Compares compression algorithms on 3000 records:

| Algorithm | Compression Ratio | Characteristics |
|-----------|-------------------|-----------------|
| **none** | 0% | Baseline |
| **snappy** | ~60% | Fast, balanced |
| **lz4** | ~65% | Medium compression, fast |
| **zstd** | ~75% | Best compression, slower |

**Metrics:** throughput, avg_latency, original_size, compressed_size, ratio, network_saved

#### **STAGE 4: PARTITIONING Testing**
Measures scalability with partition count (2 â†’ 4 â†’ 8) partitioned by `turbine_type`.

**Metrics:** throughput, scaling_factor relative to baseline, duration

#### **STAGE 5: Summary Report**
Comparative recommendations for analytics configuration.

**Usage:**
```bash
python hydro_analytics.py
```

**Requirements:** Kafka broker must be running on `localhost:9092`.

## ðŸ”§ Installing Dependencies

```bash
pip install kafka-python
```

**Dependencies:**
- `kafka-python` - Apache Kafka client
- `json`, `random`, `datetime`, `statistics`, `time` - Python built-in modules

---

## ðŸ“ˆ Analytics Recommendations

Based on test results:

### âœ… Optimal Configuration

```
Producer Config:
â”œâ”€ batch.size: 1 MB (1048576)
â”œâ”€ linger.ms: 200 ms
â”œâ”€ compression.type: zstd
â”œâ”€ acks: 1 (or all for critical data)
â””â”€ retries: 3

Topic Config:
â”œâ”€ partitions: 4-8 (depends on parallelism)
â”œâ”€ replication_factor: 1-2
â”œâ”€ retention.ms: 31536000000 (1 year for history)
â”œâ”€ segment.ms: 86400000 (1 day)
â”œâ”€ cleanup.policy: delete
â””â”€ compression.type: zstd

Consumer Config:
â”œâ”€ fetch.min.bytes: 16 KB
â”œâ”€ fetch.max.wait.ms: 500 ms
â””â”€ session.timeout.ms: 30 sec
```

### ðŸ“Š Expected Results

- **Throughput:** 1000+ records/sec with current configuration
- **Compression:** ~75% network traffic reduction (zstd)
- **Latency:** P99 < 50 ms for 1 MB batch size
- **Network Saved:** ~6-7 MB per 10,000 records

### ðŸŽ¯ Usage Scenarios

1. **Real-time Monitoring:** batch_size=16KB, linger.ms=0, compression=snappy
2. **Batch Analytics:** batch_size=1MB, linger.ms=200ms, compression=zstd
3. **Archival Storage:** compression=zstd, retention=1 year, segment=1 day

---

## ðŸ”Œ Kafka Integration

### Running Local Kafka (Docker)
```bash
# Start Zookeeper
docker run -d --name zookeeper -p 2181:2181 \
  -e ZOO_CFG_EXTRA="standaloneEnabled=false" \
  zookeeper

# Start Kafka
docker run -d --name kafka -p 9092:9092 \
  -e KAFKA_BROKER_ID=1 \
  -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
  -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092 \
  -e KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT \
  -e KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT \
  confluentinc/cp-kafka:7.0.0
```
