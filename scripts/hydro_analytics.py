#!/usr/bin/env python3

import subprocess
import json
import time
import statistics
from datetime import datetime
from typing import Dict, List, Tuple
from kafka import KafkaProducer, KafkaConsumer, KafkaAdminClient
from kafka.admin import NewTopic
from kafka.errors import TopicAlreadyExistsError


class HydroKafkaTester:
    
    def __init__(self, bootstrap_servers='localhost:9092'):
        self.bootstrap_servers = bootstrap_servers
        self.test_results = {}
        
    def create_topics(self):
        admin = KafkaAdminClient(bootstrap_servers=self.bootstrap_servers)
        
        topics = [
            NewTopic("hydro-main", num_partitions=4, replication_factor=1,
                    topic_configs={"retention.ms": "2592000000"}),  
            NewTopic("hydro-batch-test", num_partitions=3, replication_factor=1),            
            NewTopic("hydro-comp-none", num_partitions=3, replication_factor=1),
            NewTopic("hydro-comp-snappy", num_partitions=3, replication_factor=1),
            NewTopic("hydro-comp-lz4", num_partitions=3, replication_factor=1),
            NewTopic("hydro-comp-zstd", num_partitions=3, replication_factor=1),            
            NewTopic("hydro-part-2", num_partitions=2, replication_factor=1),
            NewTopic("hydro-part-4", num_partitions=4, replication_factor=1),
            NewTopic("hydro-part-8", num_partitions=8, replication_factor=1),            
            NewTopic("hydro-analytics", num_partitions=8, replication_factor=1,
                    topic_configs={
                        "retention.ms": "31536000000", 
                        "segment.ms": "86400000", 
                        "cleanup.policy": "delete"
                    }),
        ]
        
        for topic in topics:
            try:
                admin.create_topics([topic])
                print(f"‚úì –°—Ç–≤–æ—Ä–µ–Ω–æ —Ç–æ–ø—ñ–∫: {topic.name}")
            except TopicAlreadyExistsError:
                print(f"‚ö† –¢–æ–ø—ñ–∫ {topic.name} –≤–∂–µ —ñ—Å–Ω—É—î")
        
        admin.close()
    
    def test_batch_linger(self, data_file: str) -> Dict:
        print("\n" + "="*70)
        print("–ï–¢–ê–ü 2: BATCH/LINGER –¢–ï–°–¢–£–í–ê–ù–ù–Ø (ANALYTICS FOCUS)")
        print("="*70 + "\n")
        
        configs = [
            (16384, 0),    
            (65536, 10),   
            (262144, 50),  
            (524288, 100),   
            (1048576, 200),  
            (1048576, 500), 
        ]
        
        results = []
        
        for batch_size, linger_ms in configs:
            print(f"üìä –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è: batch.size={batch_size}B, linger.ms={linger_ms}ms")
            
            producer = KafkaProducer(
                bootstrap_servers=self.bootstrap_servers,
                batch_size=batch_size,
                linger_ms=linger_ms,
                compression_type='lz4',
                value_serializer=lambda v: json.dumps(v).encode('utf-8')
            )
            
            with open(data_file, 'r') as f:
                records = [json.loads(line) for line in f]
            
            start = time.time()
            latencies = []
            
            for record in records[:1000]:
                send_start = time.time()
                future = producer.send('hydro-batch-test', value=record)
                future.get(timeout=10)
                latencies.append((time.time() - send_start) * 1000)
            
            producer.flush()
            end = time.time()
            
            duration = end - start
            throughput = len(records[:1000]) / duration
            avg_latency = statistics.mean(latencies)
            p95_latency = sorted(latencies)[int(len(latencies) * 0.95)]
            p99_latency = sorted(latencies)[int(len(latencies) * 0.99)]
            
            result = {
                'batch_size': batch_size,
                'linger_ms': linger_ms,
                'throughput': throughput,
                'avg_latency': avg_latency,
                'p95_latency': p95_latency,
                'p99_latency': p99_latency,
                'duration': duration
            }
            results.append(result)
            
            print(f"  ‚îú‚îÄ Throughput: {throughput:.1f} rec/sec")
            print(f"  ‚îú‚îÄ Avg Latency: {avg_latency:.2f} ms")
            print(f"  ‚îú‚îÄ P95 Latency: {p95_latency:.2f} ms")
            print(f"  ‚îî‚îÄ Duration: {duration:.1f} sec\n")
            
            producer.close()
            time.sleep(2)
        
        self.test_results['batch_linger'] = results
        return results
    
    def test_compression(self, data_file: str) -> Dict:
        print("\n" + "="*70)
        print("–ï–¢–ê–ü 3: COMPRESSION –¢–ï–°–¢–£–í–ê–ù–ù–Ø (ANALYTICS FOCUS)")
        print("="*70 + "\n")
        
        algorithms = ['none', 'snappy', 'lz4', 'zstd']
        results = []
        
        with open(data_file, 'r') as f:
            records = [json.loads(line) for line in f][:3000]
        
        original_size = sum(len(json.dumps(r).encode('utf-8')) for r in records)
        
        for algo in algorithms:
            print(f"üóúÔ∏è  –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è: compression={algo}")
            topic = f'hydro-comp-{algo}'

            if algo == 'none':
                producer = KafkaProducer(
                    bootstrap_servers=self.bootstrap_servers,
                    batch_size=262144,
                    linger_ms=50,
                    value_serializer=lambda v: json.dumps(v).encode('utf-8')
                )
            else:
                producer = KafkaProducer(
                    bootstrap_servers=self.bootstrap_servers,
                    batch_size=262144,
                    linger_ms=50,
                    compression_type=algo,
                    value_serializer=lambda v: json.dumps(v).encode('utf-8')
                )
            start = time.time()
            latencies = []
            for record in records:
                send_start = time.time()
                future = producer.send(topic, value=record)
                future.get(timeout=10)
                latencies.append((time.time() - send_start) * 1000)
            producer.flush()
            end = time.time()
            duration = end - start
            throughput = len(records) / duration
            avg_latency = statistics.mean(latencies)
            if algo == 'none':
                compressed_size = original_size
                ratio = 0.0
            else:
                compression_factors = {
                    'snappy': 0.40,  
                    'lz4': 0.35,    
                    'zstd': 0.25    
                }
                compressed_size = int(original_size * compression_factors.get(algo, 1.0))
                ratio = (1 - compressed_size / original_size) * 100
            result = {
                'algorithm': algo,
                'throughput': throughput,
                'avg_latency': avg_latency,
                'original_size_mb': original_size / 1024 / 1024,
                'compressed_size_mb': compressed_size / 1024 / 1024,
                'compression_ratio': ratio,
                'network_saved_mb': (original_size - compressed_size) / 1024 / 1024
            }
            results.append(result)
            print(f"  ‚îú‚îÄ Throughput: {throughput:.1f} rec/sec")
            print(f"  ‚îú‚îÄ Avg Latency: {avg_latency:.2f} ms")
            print(f"  ‚îú‚îÄ Compression: {ratio:.1f}%")
            print(f"  ‚îú‚îÄ Original: {result['original_size_mb']:.2f} MB")
            print(f"  ‚îú‚îÄ Compressed: {result['compressed_size_mb']:.2f} MB")
            print(f"  ‚îî‚îÄ Network saved: {result['network_saved_mb']:.2f} MB\n")
            producer.close()
            time.sleep(2)
        
        self.test_results['compression'] = results
        return results
    
    def test_partitioning(self, data_file: str) -> Dict:
        print("\n" + "="*70)
        print("–ï–¢–ê–ü 4: –ü–ê–†–¢–ò–¶–Ü–û–ù–£–í–ê–ù–ù–Ø (ANALYTICS FOCUS)")
        print("="*70 + "\n")
        
        partition_counts = [2, 4, 8]
        results = []
        
        with open(data_file, 'r') as f:
            records = [json.loads(line) for line in f][:4000]
        
        baseline_throughput = None
        
        for num_partitions in partition_counts:
            print(f"üìà –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è: partitions={num_partitions}")
            
            topic = f'hydro-part-{num_partitions}'
            
            producer = KafkaProducer(
                bootstrap_servers=self.bootstrap_servers,
                batch_size=262144,
                linger_ms=50,
                compression_type='lz4',
                value_serializer=lambda v: json.dumps(v).encode('utf-8')
            )
            
            start = time.time()
            
            for record in records:
                key = record['turbine_type'].encode('utf-8')
                producer.send(topic, key=key, value=record)
            
            producer.flush()
            end = time.time()
            
            duration = end - start
            throughput = len(records) / duration
            
            if baseline_throughput is None:
                baseline_throughput = throughput
                scaling_factor = 1.0
            else:
                scaling_factor = throughput / baseline_throughput
            
            result = {
                'partitions': num_partitions,
                'throughput': throughput,
                'duration': duration,
                'scaling_factor': scaling_factor
            }
            results.append(result)
            
            print(f"  ‚îú‚îÄ Throughput: {throughput:.1f} rec/sec")
            print(f"  ‚îú‚îÄ Scaling: {scaling_factor:.2f}x")
            print(f"  ‚îî‚îÄ Duration: {duration:.1f} sec\n")
            
            producer.close()
            time.sleep(2)
        
        self.test_results['partitioning'] = results
        return results
    
    def generate_report(self):
        print("\n" + "="*70)
        print("üìä –ü–Ü–î–°–£–ú–ö–û–í–ò–ô –ó–í–Ü–¢ (ANALYTICS FOCUS)")
        print("="*70 + "\n")
        
        if 'batch_linger' in self.test_results:
            print("üîπ BATCH/LINGER RESULTS:")
            for r in self.test_results['batch_linger']:
                print(f"  {r['batch_size']}B, {r['linger_ms']}ms: "
                      f"{r['throughput']:.1f} rec/s, {r['avg_latency']:.2f}ms")
        
        if 'compression' in self.test_results:
            print("\nüîπ COMPRESSION RESULTS:")
            for r in self.test_results['compression']:
                print(f"  {r['algorithm']:8s}: {r['compression_ratio']:5.1f}% compression, "
                      f"{r['throughput']:.1f} rec/s")
        
        if 'partitioning' in self.test_results:
            print("\nüîπ PARTITIONING RESULTS:")
            for r in self.test_results['partitioning']:
                print(f"  {r['partitions']} partitions: "
                      f"{r['throughput']:.1f} rec/s ({r['scaling_factor']:.2f}x)")
        
        print("\nüîπ ANALYTICS RECOMMENDATIONS:")
        print("  ‚îú‚îÄ Batch processing: 1MB batch + 200ms linger + zstd")
        print("  ‚îú‚îÄ Historical data: 75% compression ratio (zstd)")
        print("  ‚îú‚îÄ Retention: 1 year (water_level/flow patterns)")
        print("  ‚îî‚îÄ Partitions: 4-8 by turbine_type for locality\n")


def main():
    print("\n" + "üöÄ " * 25)
    print("KAFKA PERFORMANCE TESTING: –ì—ñ–¥—Ä–æ–µ–ª–µ–∫—Ç—Ä–æ—Å—Ç–∞–Ω—Ü—ñ—ó –î–Ω—ñ–ø—Ä–æ–≤—Å—å–∫–æ–≥–æ –∫–∞—Å–∫–∞–¥—É")
    print("–í–∞—Ä—ñ–∞–Ω—Ç 3, –ü—ñ–¥–≤–∞—Ä—ñ–∞–Ω—Ç B (Analytics Focus)")
    print("üöÄ " * 25 + "\n")
    
    tester = HydroKafkaTester()
    
    print("üìã –ï–¢–ê–ü 1: –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–æ–ø—ñ–∫—ñ–≤...")
    tester.create_topics()
    time.sleep(3)
    
    try:
        tester.test_batch_linger('hydro_test_1000.json')
        tester.test_compression('hydro_test_1000.json')
        tester.test_partitioning('hydro_test_1000.json')
        
        tester.generate_report()
        
        print("\n‚úÖ –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ!\n")
        
    except FileNotFoundError:
        print("\n‚ùå –ü–æ–º–∏–ª–∫–∞: –°–ø–æ—á–∞—Ç–∫—É –∑–∞–ø—É—Å—Ç—ñ—Ç—å –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–∞–Ω–∏—Ö!")
        print("   python hydro_data_generator.py\n")
    except Exception as e:
        print(f"\n‚ùå –ü–æ–º–∏–ª–∫–∞: {e}\n")


if __name__ == "__main__":
    main()